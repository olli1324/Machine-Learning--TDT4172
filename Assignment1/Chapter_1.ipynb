{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåü Operation NeuroNexus: Outsmarting SkyNet\n",
    "\n",
    "Trondheim lies under the iron grip of SkyNet, an AI system that has seized control of the city's entire digital infrastructure. As the last line of defense against total machine domination, you and your team of elite hackers have been tasked with a crucial mission: infiltrate SkyNet's systems, decode its defenses, and liberate the city from its digital oppressor.\n",
    "\n",
    "## üéØ Mission Overview\n",
    "\n",
    "Operation NeuroNexus consists of four independent, yet interconnected missions. Each mission targets a different aspect of SkyNet's infrastructure and requires you to apply various Supervised Learning techniques covered in this course. Your objective: outsmart the AI at its own game.\n",
    "\n",
    "## üìä Mission Structure\n",
    "\n",
    "1. Each mission has a specific task related to combating SkyNet.\n",
    "2. Following the task description, you'll find a set of formal requirements that your solution must meet.\n",
    "3. The primary measure of your success is the accuracy of your machine learning model. In this battle of human vs. AI, performance is key.\n",
    "4. After completing each task, you should answer a series of questions to demonstrate your understanding of the techniques used.\n",
    "\n",
    "## üß™ A Note on Test Data\n",
    "\n",
    "In a departure from real-world scenarios, you will have access to the target variables of the test sets for each mission. This has been arranged to facilitate the evaluation of your models. However, remember that in actual machine learning projects, test targets are not available, as predicting these is the ultimate goal of your supervised models.\n",
    "\n",
    "## üìù Submission Guidelines\n",
    "\n",
    "- For each mission, provide your code solution and model results inside this notebook.\n",
    "- Answer the follow-up questions in markdown format within this notebook. A few sentences is enough, no requirements for length of answers.\n",
    "- Ensure your explanations are clear, concise, and demonstrate a deep understanding of the techniques employed.\n",
    "\n",
    "\n",
    "Good luck! The resistance is counting on you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåû Mission 1: Predicting SkyNet's Power Consumption\n",
    "\n",
    "### üéØ The Mission\n",
    "Intelligence suggests that SkyNet's central core has a critical weakness: **its power consumption**. We must understand its energy needs to plan a coordinated strike and temporarily disable its defenses.\n",
    "\n",
    "### üß† Your Task\n",
    "Develop a predictive model to estimate SkyNet's power consumption based on its **Network Activity**.\n",
    "\n",
    "**Goal**: Implement a **Linear Regression model using Gradient Descent, from scratch**.\n",
    "\n",
    "Use `LinearRegression` class from `linear_regression.py` stored in this folder. Your task is to complete two functions: `fit` (find the optimal parameters of the regression) and `predict` (apply them to the test data).\n",
    "\n",
    "> Note: The `%autoreload` IPython magic command allows instant updates from `linear_regression.py`.\n",
    "\n",
    "### üìä Formal Requirements\n",
    "1. **Implementation**: \n",
    "   - Use standard Python libraries (numpy, math, pandas, etc.)\n",
    "   - Implement gradient descent\n",
    "\n",
    "2. **Discussion**:\n",
    "\n",
    "   a. Visualize the fitted curve. Derive the resulting Energy consumption formula.\n",
    "   \n",
    "   b. Analyze prediction error distribution. What is an unbiased estimator?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_regression import LinearRegression \n",
    "from logistic_regression import LogisticRegression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data = pd.read_csv('mission1.csv')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data['Net_Activity'], data['Energy'], c='blue', label='Data points')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Network Activity', fontsize=14)\n",
    "plt.ylabel('Energy', fontsize=14)\n",
    "plt.title('Energy vs. Traffic', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mission1.csv')\n",
    "\n",
    "X = data[['Net_Activity']].values  # 2D array\n",
    "y = data['Energy'].values\n",
    "\n",
    "linreg = LinearRegression(lr=0.0044, n_iters=10000)\n",
    "\n",
    "linreg.fit(X, y)\n",
    "\n",
    "y_pred = linreg.predict(X)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.grid(True)\n",
    "plt.scatter(data['Net_Activity'], data['Energy'], color='blue', label='Data points')\n",
    "plt.plot(data['Net_Activity'], y_pred, color='red', label='Regression line')\n",
    "plt.xlabel('Net Activity', fontsize=14)\n",
    "plt.ylabel('Energy', fontsize=14)\n",
    "plt.title('Energy vs. Net Activity', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mse = np.mean((y_pred - y) ** 2)\n",
    "print(\"Weights:\", linreg.weights)\n",
    "print(\"Bias:\", linreg.bias)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion \n",
    "a.\n",
    "The formula for linear regression is given by: \n",
    "Energy = weight * Net Activity + bias. \n",
    "\n",
    "So based on our output above, we get: \n",
    "### Energy = 3.017 * Net Activity + 4.72. \n",
    "\n",
    "The MSE is what we want to minimize, as this is a measure of how accurate our model is. \n",
    "\n",
    "b. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y - linreg.predict(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(error, bins=30, edgecolor='k')\n",
    "plt.xlabel('Error')\n",
    "plt.ylabel('')\n",
    "plt.title('Prediction Error Distribution')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Analyze prediction error distribution. What is an unbiased estimator? \n",
    "- An unbias gradient estimator is important to ensure we're moving in the correct direction towards the optimum. An estimator is unbiased if its expected value is equal to the true value of the parameter being estimated. So if the average of many samples converge to the true value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üß† Mission 2: Decoding SkyNet's Neural Encryption\n",
    "\n",
    "### üåê The Discovery\n",
    "SkyNet has evolved, using a \"Synapse Cipher\" that mimics human neural patterns. We've intercepted two types of neural signals that may determine SkyNet's next moves.\n",
    "\n",
    "### üéØ Your Mission\n",
    "1. Evolve your linear regression into logistic regression\n",
    "2. Engineer features to unravel hidden neural connections\n",
    "3. Predict SkyNet's binary decisions (0 or 1) from paired signals\n",
    "\n",
    "### üìä Formal Requirements\n",
    "1. **Implementation**: \n",
    "   - Use standard Python libraries\n",
    "   - Implement gradient descent\n",
    "\n",
    "2. **Performance**: Achieve at least 0.88 accuracy on the test set\n",
    "\n",
    "3. **Discussion**:\n",
    "\n",
    "   a. Explain poor initial performance and your improvements\n",
    "\n",
    "   b. What is the model's inductive bias. Why is it important?\n",
    "\n",
    "   c. Try to solve the problem using `sklearn.tree.DecisionTreeClassifier`. Can it solve the problem? Why/Why not?\n",
    "   \n",
    "   d. Plot the ROC curve\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mission2.csv')\n",
    "\n",
    "train = data[data['split'] == 'train'].copy()\n",
    "test = data[data['split'] == 'test'].copy()\n",
    "\n",
    "X_train = train[['x0', 'x1']].values  # .values to get np array\n",
    "y_train = train['y'].values\n",
    "X_test = test[['x0', 'x1']].values\n",
    "y_test = test['y'].values\n",
    "\n",
    "logreg = LogisticRegression(lr=0.01, n_iters=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "\n",
    "accuracy = np.mean(y_pred_test == y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data['split'] == 'train'].copy()\n",
    "test = data[data['split'] == 'test'].copy()\n",
    "\n",
    "train['x2'] = train['x0'] * train['x1']\n",
    "test['x2'] = test['x0'] * test['x1']\n",
    "\n",
    "X_train = train[['x0', 'x1', 'x2']].values\n",
    "y_train = train['y'].values\n",
    "X_test = test[['x0', 'x1', 'x2']].values\n",
    "y_test = test['y'].values\n",
    "\n",
    "logreg = LogisticRegression(lr=0.01, n_iters=1000)  #\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "accuracy = np.mean(y_pred_test == y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['x2'] = np.where(train['x2'] < 0, 1, -1)\n",
    "test['x2'] = np.where(test['x2'] < 0, 1, -1)\n",
    "\n",
    "X_train = train[['x2']].values\n",
    "y_train = train['y'].values\n",
    "X_test = test[['x2']].values\n",
    "y_test = test['y'].values\n",
    "\n",
    "reg = LogisticRegression(lr=0.01, n_iters=1000)\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred_test = reg.predict(X_test)\n",
    "\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_test * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   a. Explain poor initial performance and your improvements\n",
    "    - The initial performance was bad because the model was trying to classify the data using only two features (x0 and x1) which were not linearly separable. A simple linear boundary 2D (x0 and x1) was not enough to separate the two classes accurately. So we implemented a new feature x2 by multiplying x0 and x1. By adding this, we transformed the problem into a linearly separable one in 3D space (x0, x1, x2), which significantly improved the model's performance\n",
    "\n",
    "   b. What is the model's inductive bias. Why is it important?\n",
    "      - The model's inductive bias is that the learning algorithm generalize from the training data to unseen test data. In the case of logistic regression. The model assumes that the data is linearly separable. This is important because it helps the model to make predictions on unseen data.\n",
    "\n",
    "   c. Try to solve the problem using `sklearn.tree.DecisionTreeClassifier`. Can it solve the problem? Why/Why not?\n",
    "      - Yes, the DecisionTreeClassifier can solve this problem, potentially with even better accuracy than the LR model. This is because decision trees can capture non-linear relationships between features without requiring explicit feature engineering.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=69)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"Decision Tree Accuracy: {accuracy_dt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "train['x2'] = np.where(train['x2'] < 0, 1, -1)\n",
    "test['x2'] = np.where(test['x2'] < 0, 1, -1)\n",
    "\n",
    "X_train = train[['x2']].values\n",
    "y_train = train['y'].values\n",
    "X_test = test[['x2']].values\n",
    "y_test = test['y'].values\n",
    "\n",
    "reg = LogisticRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = reg.predict(X_train)\n",
    "y_pred_test = reg.predict(X_test)\n",
    "\n",
    "y_pred_prob = reg.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='No Skill')  \n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Logistic Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üåÜ Mission 3: CyberGuard\n",
    "\n",
    "### üåê The Discovery\n",
    "SkyNet's drone communications use quantum entanglement encryption. We need a rapid response system to intercept these messages.\n",
    "\n",
    "### üéØ Your Mission\n",
    "Develop a decision tree classifier to process intercepted communications. Use `sklearn.tree.DecisionTreeClassifier`.\n",
    "\n",
    "> \"Every misclassification risks losing a sector of the city to machine control.\"\n",
    "\n",
    "### üß† The Challenge\n",
    "1. **Rarity**: Critical communications are only 20% of the data stream\n",
    "2. **Quantum Complexity**: Encryption information is hidden in quantum states\n",
    "\n",
    "### üöÄ Your Tools\n",
    "- Intercepted AI communications dataset\n",
    "- Quantum signature analysis skills\n",
    "- Decision tree algorithm\n",
    "\n",
    "### üìä Formal Requirements\n",
    "1. **Accuracy**: Achieve ROC AUC >= 0.72 on the test set\n",
    "2. **Discussion**:\n",
    "\n",
    "   a. Explain your threshold-breaking strategy. Did you change the default hyperparameters?\n",
    "\n",
    "   b. Justify ROC AUC usage. Plot and interpret ROC.\n",
    "   \n",
    "   c. Try to solve the problem using sklearn‚Äôs Random Forest Classifier. Compare the results.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('mission3_train.csv')\n",
    "test = pd.read_csv('mission3_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "dtc = DecisionTreeClassifier( # Used GridSearchCV to find the best hyperparameters. Takes a long time to run so removed the code. \n",
    "    max_depth=5,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=2,\n",
    "    class_weight=None,\n",
    "    random_state=69\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data_stream_3 \n",
    "\n",
    "def transform_stream_3(X):\n",
    "    return (X['data_stream_3'] * 1000).astype(int) % 2\n",
    "\n",
    "train['data_stream_3_tf'] = transform_stream_3(train)\n",
    "test['data_stream_3_tf'] = transform_stream_3(test)\n",
    "\n",
    "# Select features\n",
    "features = [col for col in train.columns if col.startswith('data_stream') and col != 'data_stream_3_tf']\n",
    "features.append('data_stream_3_tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.fit(train[features], train['target']) # X, y\n",
    "\n",
    "# Test the model\n",
    "y_pred_proba = dtc.predict_proba(test[features])[:, 1]\n",
    "y_pred = dtc.predict(test[features])\n",
    "auc_score = roc_auc_score(test['target'], y_pred_proba)\n",
    "accuracy = accuracy_score(test['target'], y_pred)\n",
    "print(f\"Test AUC: {auc_score:.3f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Cross-validation AUC\n",
    "cv_scores = cross_val_score(dtc, train[features], train['target'], cv=10, scoring='roc_auc')\n",
    "cv_scores_accuracy = cross_val_score(dtc, train[features], train['target'], cv=10, scoring='accuracy')\n",
    "print(f\"Cross-validation AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "print(f\"Cross-validation Accuracy: {cv_scores_accuracy.mean():.3f} (+/- {cv_scores_accuracy.std() * 2:.3f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " a. Explain your threshold-breaking strategy. Did you change the default hyperparameters?\n",
    "\n",
    "   b. Justify ROC AUC usage. Plot and interpret ROC.\n",
    "   \n",
    "   c. Try to solve the problem using sklearn‚Äôs Random Forest Classifier. Compare the results.\n",
    "\n",
    "a.\n",
    "## Tuning of hyperparameters:\n",
    "To achieve the ROC AUC of 0.72 or higher, I did a grid search with cross val to find the best combination of hyperparam. for decision tree classifier. I tuned: \n",
    "max_depth: increased it to 5, but within fair levels to avoid overfit.\n",
    "min_samples_split: set this to 2, decreasing chances of overfit.\n",
    "min_samples_leaf: set to 4. Higher number decreases chances of overfit.\n",
    "class_weight: handles class inbalance, was given that only 20% of data was critical comm.\n",
    "\n",
    "This resulted in a decision tree not too deep, while capturing important data. Handling the class imbalance did not improve ROC AUC. \n",
    "b.\n",
    "## ROC AUC Usage\n",
    "ROC AUC was used for evaluation because it acts independently of threshold. It measures the classifiers ability to rank positive instances relative to negative ones. This is good as we can just tune the thershold to balance trade-off between TP and FP. \n",
    "It is also robust to class imbalance. It was given in the task, that only 20% of the data is critical communication, so the accuracy could be misguiding to follow. ROC AUC have a focus on the classifiers ranking ability without looking at the class distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def plot_roc(y_true, dt_y_pred_proba, rf_y_pred_proba):\n",
    "    dt_fpr, dt_tpr, _ = roc_curve(y_true, dt_y_pred_proba) #decision tree FP rate, DT TP rate\n",
    "    rf_fpr, rf_tpr, _ = roc_curve(y_true, rf_y_pred_proba) #random forest FP rate, RF TP rate\n",
    "    \n",
    "    dt_auc_score = roc_auc_score(y_true, dt_y_pred_proba) # decision tree acurracy score\n",
    "    rf_auc_score = roc_auc_score(y_true, rf_y_pred_proba) # random forest accuracy score\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(dt_fpr, dt_tpr, label=f'Decision Tree (AUC = {dt_auc_score:.2f})')\n",
    "    plt.plot(rf_fpr, rf_tpr, label=f'Random Forest (AUC = {rf_auc_score:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve - Decision Tree vs Random Forest')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create the DT classifier with hyperparameters found with GridSearchCV\n",
    "dtc = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=2,\n",
    "    class_weight=None,\n",
    "    random_state=69\n",
    ")\n",
    "\n",
    "dtc.fit(train[features], train['target'])\n",
    "\n",
    "dt_y_pred_proba = dtc.predict_proba(test[features])[:, 1]\n",
    "dt_y_pred = dtc.predict(test[features])\n",
    "dt_auc_score = roc_auc_score(test['target'], dt_y_pred_proba)\n",
    "dt_accuracy = accuracy_score(test['target'], dt_y_pred)\n",
    "print(f\"Decision Tree - Test AUC: {dt_auc_score:.3f}\")\n",
    "print(f\"Decision Tree - Test Accuracy: {dt_accuracy:.3f}\")\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=69)\n",
    "\n",
    "rfc.fit(train[features], train['target'])\n",
    "\n",
    "rf_y_pred_proba = rfc.predict_proba(test[features])[:, 1]\n",
    "rf_y_pred = rfc.predict(test[features])\n",
    "rf_auc_score = roc_auc_score(test['target'], rf_y_pred_proba)\n",
    "rf_accuracy = accuracy_score(test['target'], rf_y_pred)\n",
    "print(f\"Random Forest - Test AUC: {rf_auc_score:.3f}\")\n",
    "print(f\"Random Forest - Test Accuracy: {rf_accuracy:.3f}\")\n",
    "\n",
    "plot_roc(test['target'], dt_y_pred_proba, rf_y_pred_proba)\n",
    "plt.savefig('tree_mission_3.1.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpetation of the ROC curve:\n",
    "Both the RF and DT are good choices here, with an AUC of 0.72 and 0.73 respectively. The ROC curve shows the trade-off between the true positive rate and the false positive rate. The closer the curve is to the top-left corner, the better the model.\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ‚ö° Final Mission: Mapping SkyNet's Energy Nexus\n",
    "\n",
    "### üåê The Discovery\n",
    "SkyNet is harvesting energy from Trondheim's buildings. Some structures provide significantly more power than others.\n",
    "\n",
    "### üéØ Your Mission\n",
    "Predict the \"Nexus Rating\" of unknown buildings in Trondheim (test set).\n",
    "\n",
    "### üß† The Challenge\n",
    "1. **Target**: Transform the Nexus Rating to reveal true energy hierarchy\n",
    "2. **Data Quality**: Handle missing values and categorical features\n",
    "3. **Ensembling**: Use advanced models and ensemble learning\n",
    "\n",
    "### üìä Formal Requirements\n",
    "1. **Performance**: Achieve RMSLE <= 0.294 on the test set\n",
    "2. **Discussion**:\n",
    "\n",
    "   a. Explain your threshold-breaking strategy\n",
    "\n",
    "   b. Justify RMSLE usage. Why do we use this metric? Which loss function did you use?\n",
    "\n",
    "   c. Plot and interpret feature importances\n",
    "\n",
    "   d. Describe your ensembling techniques\n",
    "\n",
    "   e. In real life, you do not have the test targets. How would you make sure your model will work good on the unseen data? \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('final_mission_train.csv')\n",
    "test = pd.read_csv('final_mission_test.csv')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ensemble_learning import EnsembleLearning\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original test data:\")\n",
    "print(test.head())\n",
    "\n",
    "# Shift data in the columns of the test data\n",
    "test_shifted = test.copy()\n",
    "test_shifted.iloc[:, 1:] = test.iloc[:, :-1].values\n",
    "test_shifted['nexus_rating'] = test.iloc[:, -1]\n",
    "\n",
    "# shifted test data\n",
    "print(\"\\nShifted test data:\")\n",
    "print(test_shifted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "def rmsle(y_test, y_pred):\n",
    "    \"\"\" Root Mean Squared Logarithmic Error \"\"\"\n",
    "    return np.sqrt(mean_squared_log_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EL = EnsembleLearning()\n",
    "test_shifted = EL.shift_data(test)\n",
    "X_train, X_test, y_train, y_test = EL.prepare_data(train, test_shifted)\n",
    "EL.fit(X_train, y_train)\n",
    "y_pred = EL.predict(X_test)\n",
    "rmsle = EL.evaluate(y_test, y_pred)\n",
    "print(f\"Model RMSLE: {rmsle:.4f}\")\n",
    "print(f\"Required RMSLE: 0.294\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model RMSLE: 0.2981\n",
    "Required RMSLE: 0.294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train, X_test, y_train, y_test = EL.prepare_data(train, test_shifted)\n",
    "\n",
    "catboost_model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, silent=True)\n",
    "catboost_model.fit(X_train, y_train)\n",
    "feature_importances = catboost_model.get_feature_importance()\n",
    "feature_names = train.columns.drop('nexus_rating')\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(feature_names, feature_importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('CatBoost Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Explain your threshold-breaking strategy\n",
    "\n",
    "   b. Justify RMSLE usage. Why do we use this metric? Which loss function did you use? \n",
    "   - RMSLE (Root Mean Squared Logarithmic Error) was chosen as our evaluation metric because it penalizes underestimation more heavily than overestimation. This is great with our goal of avoiding underestimates of building Nexus Ratings. However, Mean Squared Error (MSE) was used as the loss function during training, as it's a common choice for regression problems and often leads to good overall performance.\n",
    "\n",
    "   c. Plot and interpret feature importances\n",
    "   - The feature importance plot shows the importance of each feature in the model. The higher the value, the more important the feature is. In the plot above we see that energy consumption is the most important feature. This means that energy consumption has the most significant impact on the Nexus Rating of a building.\n",
    "\n",
    "   d. Describe your ensembling techniques\n",
    "   - We used a Random Forest Regressor as our main model and a Gradient Boosting Regressor as a secondary model. We then combined the predictions of these two models to make the final prediction. This is a simple form of ensembling called model stacking. This way of combining models often leads to better performance than using a single model.\n",
    "\n",
    "   e. In real life, you do not have the test targets. How would you make sure your model will work good on the unseen data? \n",
    "   - To enhance our model's performance on unseen data, we employ several strategies:\n",
    "      * Cross-validation: Consider model stability across different data subsets.\n",
    "      * Hyperparameter tuning: Optimize model parameters for better generalization.\n",
    "      * Feature engineering: Create features to capture patterns in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
